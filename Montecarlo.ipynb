{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24cc32d2-8d71-4403-81b1-2958608292d7",
   "metadata": {},
   "source": [
    "# **Montecarlo**\n",
    "----\n",
    "\n",
    "## Goals of this notebook:\n",
    "\n",
    "1. Make sure that the most basic environment (HitStand) properly works.\n",
    "2. Deploy main plotting utilities.\n",
    "3. Execute a simple experiment to make sure that my hand-made Montecarlo-based algorithms (for both prediction and control) are mistake proof.\n",
    "4. Define methods to accelerate multiple episode executions in later notebooks.\n",
    "5. Showcase agents' common efficiency-oriented capabilities (e.g.: Q/V-table storage and agent storage).\n",
    "\n",
    "---\n",
    "## Library imports\n",
    "\n",
    "#### 1. RL libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d26b02c8-84fc-4563-94bd-7bd8b7ceb9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import environments\n",
    "from agents.agents import MonteCarloPredictor, MontecarloController, OffPolicyMontecarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d4d35-3d33-40d1-a921-d1861f62f69c",
   "metadata": {},
   "source": [
    "#### 2. Data aggregation and matrix operation libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "865c59cf-1cf2-414f-9e22-46b36116b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436f118-3f8f-4b7e-abfe-03f5fadc6f81",
   "metadata": {},
   "source": [
    "#### 3. Plotting libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b0be9f3-8672-44f3-ad78-b00bfa1d377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# Commands to tweak notebook layout:\n",
    "from sys import maxsize\n",
    "np.set_printoptions(threshold=maxsize)\n",
    "plt.style.use('seaborn-pastel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e25f02-2dbc-42d7-8e21-818a896d2309",
   "metadata": {},
   "source": [
    "#### 4. Statistical analysis tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2287af-abda-462c-beeb-6d00736ee94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410e9ea-3cd9-454c-b36e-fe51eb05e210",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Common Plotting utilities:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f94afc4-0935-4bf5-abf5-43c8333c91da",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Basic enviroment presentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a4f001-843c-4de0-a935-44287a200b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI gym-like enviroment instance creation:\n",
    "env = environments.make('hitstand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e60d22ef-2cd0-4215-b797-773377bd75f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 6 decks (with replacement after each episode)\n",
      "2. Dealer stands on soft 17\n",
      "3. No Double Down\n",
      "4. No split\n",
      "5. No insurance offered\n",
      "6. No surrender\n",
      "7. Natural Blackjack 3:2\n"
     ]
    }
   ],
   "source": [
    "print(env)\n",
    "# Main rules:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560bdb54-9c0a-4384-8a85-ac72d9e50afc",
   "metadata": {},
   "source": [
    "**Code note**: although natural Blackjack triggers a 3:2 payoff, it will never be computed as Blackjack states entail automatic state transition (the agent receives two cards and just needs to wait for hand resolution).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cae5209-4132-4fa7-a296-b5459760dbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1: 'The House beats Jack',\n",
       " 0: 'Draw/not terminal',\n",
       " 1: 'Jack beats the House',\n",
       " 1.5: 'Blackjack for Jack'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_space_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ae162f6-bc23-4619-92e8-39c497e9ef63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: \"Player's total\", 1: \"Dealer's card value\", 2: 'Player has got usable ace'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space_description\n",
    "# Every observation consists of a 3-element tuple, the description for each element and their positions are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89adf5e-25e0-4443-a23e-4a2282fc7d85",
   "metadata": {},
   "source": [
    "A \"usable ace\" means the ace value is 11, instead of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "842c98c2-3a51-4b0d-9c7b-f64ebead824e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Observation space ranges from [4 2 0] to [30 26  1]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Observation space ranges from {0} to {1}'.format(env.observation_space_low, env.observation_space_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf43a65-45e1-40e7-82ab-66a091389a42",
   "metadata": {},
   "source": [
    "**Comments on the observation space definition**:\n",
    "\n",
    "- Lower bounds: Player's total value is bounded from below at 4, as any potential smaller value would imply the use of an ace (and its value is 11 at the start of the hand); Dealer's is bounded from below at 2, as he/she is only handed in one card.\n",
    "\n",
    "- Upper bounds: those reach the maximum value a player/dealer might obtain. However, any state that goes over 21 is not computationally taken into account; they are terminal states and final reward is associated to both the last executed action and the state from which transition to terminal state is taking place. Therefore, the Q/V-table values for terminal states remain 0 over the whole experiments.\n",
    "\n",
    "- Player has got usable ace: a binary encoding is used to indicate whether or not the player has got a usable ace (0=False, 1=True).\n",
    "\n",
    "----\n",
    "\n",
    "## Experiment Definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "726c4ebd-1492-4745-9b4b-fc3524c35b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(environment, agent, episodes, show, save=None, collect_rewards=None, train=True):\n",
    "    \"\"\"\n",
    "    environment: instance of the environment to execute.\n",
    "    agent: instance of the agent that will interact with the environment.\n",
    "    show: integer. It indicates how often episodes are printed as text.\n",
    "    save: integer. It indicates how often the V/Q-table is permanently persisted as a pickle object. None = no persistance at all.\n",
    "    collect_rewards: It indicates every so many episodes rewards are averaged. 1 = return every reward.\n",
    "    train: bool. If True, the agent learns by updating its V/Q-table; if False, the agent just executes a defined policy.\n",
    "    \n",
    "    returns: a list containing the average rewards over the whole experiment.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    average_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        if (episode+1) % show ==0:\n",
    "            print('Episode {0}:'.format(episode+1))\n",
    "            env.render()\n",
    "\n",
    "        state, reward, terminal, _ = env.reset()\n",
    "        while not terminal:\n",
    "            action = agent.follow_policy(state)\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            if train:\n",
    "                agent.evaluate_state(state, reward, terminal, action)   \n",
    "                \n",
    "            state=next_state\n",
    "            \n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if save:\n",
    "            if (episode+1) % save == 0:\n",
    "                agent.save_table()\n",
    "                \n",
    "        if collect_rewards:\n",
    "            if (episode+1) % collect_rewards == 0:\n",
    "                average_reward = sum(rewards[-collect_rewards:])/collect_rewards\n",
    "                average_rewards.append(average_reward)\n",
    "    \n",
    "    return average_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fef780-5a61-4ac6-9543-3681c527f332",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Agent Deployments\n",
    "\n",
    "### 1. Montecarlo Prediction:\n",
    "\n",
    "The agent assesses a policy and estimates its value. There is no learning.\n",
    "\n",
    "I use **every visit** Montecarlo. For the problem at hand, there is no difference between first visit and every visit MC, because the likelihood to visit the same state in the same episode is 0%: the agent only keeps the same value when, having obtained a combination of ace and a card between 2 and 9, it hits and receives a 10-point card. However, that triggers a transition from a usable-ace state to a non-usable-ace state.\n",
    "\n",
    "**Code note**: before instantiating an agent, its class must be subclassed in order to override the not implemented \"follow_policy\" method. This is the idea I have come up with to define policies for a given agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9724ed13-4690-4c26-af0b-106d85dcf93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deterministic(MonteCarloPredictor):\n",
    "    \n",
    "    def follow_policy(self, observation, *args):\n",
    "        if observation[0] > 17:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adc4d868-fc37-4ed4-a5ca-46f913f2946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic_agent = Deterministic(env)\n",
    "# Environment instance is passed as an argument; it helps define a proper size for V/Q-table based on the environment's features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9058ecc1-731d-4db1-8a1e-6d7fd817d7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'discount_rate': 1, 'learning_rate': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deterministic_agent.hyperparams\n",
    "# The agent have the following default parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245106cd-7272-48fc-a016-0aac4b1c1f8d",
   "metadata": {},
   "source": [
    "**Code note**:\n",
    "If **learning rate** value is set to None, it means that the learning rate is defined as $\\frac{1}{t}$ (where $t$ is the number of times a state has been visited). It is the default way to push the average towards its true value incrementally and it satisfies the Robbins-Monro conditions:\n",
    "\n",
    "$$\\sum_{t=0}^{\\infty}\\frac{1}{t} = \\infty$$\n",
    "$$\\sum_{t=0}^{\\infty}\\frac{1}{t^2} < \\infty$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
