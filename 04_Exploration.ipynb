{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e8c518-33ae-4535-9605-4e97ea2dca2b",
   "metadata": {},
   "source": [
    "# **Exploration vs Exploitation dilemma**\n",
    "----\n",
    "\n",
    "## Goals of this notebook:\n",
    "\n",
    "1. Discuss some exploration strategies that might be brought to bear when training agents.\n",
    "---\n",
    "## Library imports\n",
    "\n",
    "#### 1. RL libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d09404a-7b62-4294-a82e-a8b36e4166a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import environments\n",
    "from agents.agents import TDLambdaPredictor, WatkinsLambda, Sarsa,\\\n",
    "                          QLearning, SarsaLambda, MonteCarloPredictor,\\\n",
    "                          MontecarloController, OffPolicyMontecarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080e0aa-e2c0-43dc-96e2-59ce0a7a81e3",
   "metadata": {},
   "source": [
    "#### 2. Data aggregation and matrix operation libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc2aa80-6eea-42ac-b4cb-2a3d4d6073f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413faab7-07f7-4e9c-a9ef-7153ce657d4f",
   "metadata": {},
   "source": [
    "#### 3. Plotting libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c37907-068d-4470-bba5-f27dad899e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib.ticker import FormatStrFormatter, ScalarFormatter, FuncFormatter\n",
    "import seaborn as sns\n",
    "# Commands to tweak notebook layout:\n",
    "from sys import maxsize\n",
    "np.set_printoptions(threshold=maxsize)\n",
    "plt.style.use('seaborn-pastel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863e7677-aec6-4f5c-b847-a3f53a84fea0",
   "metadata": {},
   "source": [
    "#### 4. Parallel programming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a0a35cf-9c6f-4925-babb-354bdd19a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de795f1f-7362-4b21-94a6-6d66842dce94",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Common plotting utilities:\n",
    "\n",
    "#### Average reward time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dc6d8e7-571f-491a-8ae6-a0da324ca7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_time_series(X,Y, far=True, lower=None, upper=None):\n",
    "    \"\"\"\n",
    "    X: a range of numbers starting from the sample size number to the total number of episode + 1\n",
    "       (to make it inclusive) and incrementing by sample size steps.\n",
    "    Y: average rewards\n",
    "    far: bool. If False the plots zooms in.\n",
    "    lower: the lower bound of average rewards. Same length than Y.\n",
    "    upper: the upper bound of average rewards. Same length than Y.\n",
    "    \n",
    "    returns: the plot container instance\n",
    "    \"\"\"\n",
    "    # domain redefinition to adjust how many x-ticks are displayed dinamically\n",
    "    max_domain = X[-1]\n",
    "    multiplier = len(str(max_domain)) - 2\n",
    "    step = int(str(max_domain)[0]) * 10**multiplier\n",
    "    domain = np.arange(X[0], max_domain + step, step)\n",
    "    \n",
    "    if far:\n",
    "        ylim = (-1, 1)\n",
    "        yticks= np.arange(-1,1.05,0.05)\n",
    "    else:\n",
    "        max_value = max(0, max(Y))\n",
    "        ylim = (min(Y)-0.1, max_value)\n",
    "        yticks = np.round(np.arange(min(Y)-0.1, max_value+0.1, 0.01),2)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    \n",
    "    style_dict = {'xlim': (X[0], max_domain),\n",
    "                  'xticks': domain,\n",
    "                  'xticklabels': domain,\n",
    "                  'xlabel': 'Episodes',\n",
    "                  'ylim': ylim,\n",
    "                  'yticks': yticks,\n",
    "                  'yticklabels': yticks,\n",
    "                  'title': 'Average reward over last {0:,.0f} episodes'.format(X[0])\n",
    "                 }\n",
    "    \n",
    "    ax = fig.add_subplot(1,1,1, **style_dict)\n",
    "    ax.grid()\n",
    "    #Axis formatters:\n",
    "    formatter = ScalarFormatter(useMathText=True)\n",
    "    #Scientific value display when the total number of episodes is over 1M:\n",
    "    formatter.set_scientific(True)\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('% 1.2f'))\n",
    "    # in and out of the money border:\n",
    "    ax.plot(domain.ravel(), np.zeros_like(domain).ravel(), color='red')\n",
    "    \n",
    "    #if bounds of ci are passed as arguments, plot them:\n",
    "    if lower and upper:\n",
    "        ax.fill_between(X, lower, upper, alpha=0.1, color='green')\n",
    "    \n",
    "    ax.plot(X,Y)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c6821-b7fe-4e2c-82bd-e8db8dc35c45",
   "metadata": {},
   "source": [
    "#### Density plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a9304ec-a298-40d0-bf16-2186e748ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density(data):\n",
    "    \"\"\"\n",
    "    data: a Panda Series containing every sample of average rewards\n",
    "    \n",
    "    returns: both histogram and boxplot container instances\n",
    "    \"\"\"\n",
    "    title = 'Average reward sampling: $\\overline{{X}}$={0:.2f}, s={1:.2f}, n={2:,.0f}, num of samples={3:,.0f}'\\\n",
    "            .format(data.mean(),\n",
    "                    data.std(),\n",
    "                    data.index.values[0],\n",
    "                    len(data))\n",
    "    \n",
    "    fig, (ax_box, ax_hist) = plt.subplots(2, sharex=True, figsize=(20,10), gridspec_kw={\"height_ratios\": (.15, .85)}) \n",
    "    fig.suptitle(t=title, fontsize=16, x=0.5, y=1.05)\n",
    "    \n",
    "    sns.boxplot(data=data,x=data, ax=ax_box)\n",
    "    sns.histplot(data=data, x=data, ax=ax_hist, bins=20, kde=True)\n",
    "    \n",
    "    #Delimiting tails:\n",
    "    ax_hist.axvline(x=np.percentile(data,[2.5]), label='2.5th percentile', c='r')\n",
    "    ax_hist.axvline(x=np.percentile(data,[97.5]), label='97.5th percentile', c='r')\n",
    "    ax_hist.legend()\n",
    "    ax_hist.set_xlabel('Average Reward')\n",
    "    \n",
    "    return ax_box, ax_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e547e1c5-2565-49c8-b4d3-8b90d45cad3d",
   "metadata": {},
   "source": [
    "#### Value Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ea21eed-b737-4dbf-bdc6-265c6aa5a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_v_func(table, title):\n",
    "    \"\"\"\n",
    "    table: agent's V-table (if agents has a Q-table, it must be transformed before being passed as an argument for this function)\n",
    "    title: str. A title for the plot\n",
    "    \n",
    "    returns: the plot container instance\n",
    "    \"\"\"\n",
    "    X = np.linspace(1,10,10)\n",
    "    Y = np.linspace(12, 20,9)\n",
    "    Xm, Ym = np.meshgrid(X, Y)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.suptitle(t=title, fontsize=16, x=0.5, y=1.05)\n",
    "    \n",
    "    common_style_dict = {'xlim': (X[0], X[-1]),\n",
    "                        'xticks': X,\n",
    "                        'xticklabels': ['{:.0f}'.format(value) for value in list(X)[1:]] + ['A'],\n",
    "                        'xlabel': 'Dealer\\'s Card',\n",
    "                        'ylim': (Y[0], Y[-1]),\n",
    "                        'yticks': Y,\n",
    "                        'yticklabels': Y,\n",
    "                         'ylabel': 'Player\\'s Total',\n",
    "                        'zlim': (-1, 1.5),\n",
    "                        'zticks': np.arange(-1, 1.8, 0.2),\n",
    "                        'zticklabels': np.arange(-1, 1.8, 0.2),\n",
    "                        }\n",
    "    \n",
    "    #Not usable ace-related states:\n",
    "    ax = fig.add_subplot(1,2,1, projection='3d', title='No usable Ace', **common_style_dict)\n",
    "    surface_1 = ax.plot_surface(Xm, Ym, table[8:17,:10,0], cmap=plt.get_cmap('bwr'), vmin=-1, vmax=1)\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('% 1.0f'))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('% 1.1f'))\n",
    "    ax.view_init(ax.elev, -120)\n",
    "    \n",
    "    #Usable ace-related states:\n",
    "    ax = fig.add_subplot(1,2,2, projection='3d', title=' Usable Ace', **common_style_dict)\n",
    "    surface_2 = ax.plot_surface(Xm, Ym, table[8:17,:10,1], cmap=plt.get_cmap('bwr'), vmin=-1, vmax=1)\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('% 1.0f'))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('% 1.1f'))\n",
    "    ax.view_init(ax.elev, -120)\n",
    "    \n",
    "    #Colour bar:\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.35, 0.01, 0.5])\n",
    "    fig.colorbar(surface_1,shrink=0.2, aspect=15, cax=cbar_ax)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0b4d5f-8cbb-4607-b73e-d04620800812",
   "metadata": {},
   "source": [
    "#### Policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60780b4c-7d72-4d42-adcd-2e4ccc75243a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_policy(table, title):\n",
    "    \"\"\"\n",
    "    table: matrix containing action indexes (it must have n-1 dimensions with respect to the agent's Q-table)\n",
    "    title: str. A title for the plot\n",
    "    \n",
    "    returns: the plot container instance\n",
    "    \"\"\"\n",
    "    #tick label transformer (used for the colour bar):\n",
    "    def format_tick(x, pos):\n",
    "        if x == 0:\n",
    "            return '{0}-STAND'.format(str(x))\n",
    "        elif x == 1:\n",
    "            return '{0}-HIT'.format(str(x))\n",
    "    \n",
    "    X = np.arange(-.5,9.5,1)\n",
    "    Y = np.arange(0.5, 11.5,1)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.suptitle(t=title, fontsize=16, x=0.5, y=1.05)\n",
    "    \n",
    "    common_style_dict = {'xlim': (-.5, 9.5),\n",
    "                        'xticks': X,\n",
    "                        'xticklabels': ['{:.0f}'.format(value) for value in range(2,11,1)] + ['A'],\n",
    "                        'xlabel': 'Dealer\\'s Card',\n",
    "                        'yticks': Y,\n",
    "                        'yticklabels': np.arange(11,22,1),\n",
    "                        'ylabel': 'Player\\'s Total'\n",
    "                        }\n",
    "    #I only plot those states wherein there is risk of going bust after hitting once more and, for the sake of symmetry,\n",
    "    #I manually modify those non-existing usable-ace-related states:\n",
    "    table[:8,:,1] = 1\n",
    "    \n",
    "    #Not usable ace-related states:\n",
    "    ax = fig.add_subplot(1,2,1, title='No usable Ace', **common_style_dict)\n",
    "    im = ax.imshow(table[7:18,:10,0], cmap=plt.get_cmap('bwr'), vmin=0, vmax=1, alpha=0.5)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True,color='w', linestyle='-', linewidth=1, axis='both')\n",
    "    ##Little tweak for tick labels' position:\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_horizontalalignment('left')\n",
    "\n",
    "    for label in ax.get_yticklabels():\n",
    "        label.set_verticalalignment('top')\n",
    "    \n",
    "    #Usable ace-related states:\n",
    "    ax = fig.add_subplot(1,2,2, title=' Usable Ace', **common_style_dict)\n",
    "    ax.imshow(table[7:18,:10,1],cmap=plt.get_cmap('bwr'), vmin=0, vmax=1, alpha=0.5)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True,color='w', linestyle='-', linewidth=1, axis='both')\n",
    "    ##Little tweak for tick labels' position:\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_horizontalalignment('left')\n",
    "    \n",
    "    #Colour bar definition\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    original_cmap = plt.get_cmap('bwr')\n",
    "    \n",
    "    #Converting a multiple-colour cmap into 2-colour cmap\n",
    "    cmap = ListedColormap([original_cmap(0), original_cmap(original_cmap.N)])\n",
    "    bounds = [0, 0.5, 1]\n",
    "    norm = BoundaryNorm(bounds, original_cmap.N)    \n",
    "    cbar_ax = fig.add_axes([0.85, 0.05, 0.01, 0.85])\n",
    "    \n",
    "    fig.colorbar(cm.ScalarMappable(cmap=cmap, norm=norm),\n",
    "                 shrink=0.2,\n",
    "                 aspect=15,\n",
    "                 cax=cbar_ax,\n",
    "                 boundaries= [0] + bounds + [2],  \n",
    "                 extend='both',\n",
    "                 ticks=[0,1],\n",
    "                 format = FuncFormatter(format_tick),\n",
    "                 spacing='proportional',\n",
    "                 orientation='vertical',\n",
    "                 alpha= 0.5)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c8d081-8295-43f1-86dc-81ad2b587857",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Experiment Definition:\n",
    "\n",
    "#### Montecarlo method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c53afc4-2f99-49b0-9054-9aee903caf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(environment, agent, episodes, show, save=None, collect_rewards=None, train=True):\n",
    "    \"\"\"\n",
    "    environment: instance of the environment to execute.\n",
    "    agent: instance of the agent that will interact with the environment.\n",
    "    show: integer. It indicates how often episodes are printed as text(it helps track the existence of bugs in the game itself).\n",
    "    save: integer. It indicates how often the V/Q-table is permanently persisted as a pickle object. None = no persistence at all.\n",
    "    collect_rewards: It indicates every so many episodes rewards are averaged. 1 = return every reward.\n",
    "    train: bool. If True, the agent learns by updating its V/Q-table; if False, the agent just executes a defined policy.\n",
    "    \n",
    "    returns: a list containing all average rewards computed throughout the whole experiment.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    average_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        if (episode+1) % show ==0:\n",
    "            print('Episode {0}:'.format(episode+1))\n",
    "            env.render()\n",
    "\n",
    "        state, reward, terminal, _ = env.reset()\n",
    "        while not terminal:\n",
    "            action = agent.follow_policy(state)\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            if train:\n",
    "                agent.evaluate_state(state, reward, terminal, action)   \n",
    "                \n",
    "            state=next_state\n",
    "            \n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if save:\n",
    "            if (episode+1) % save == 0:\n",
    "                agent.save_table()\n",
    "                \n",
    "        if collect_rewards:\n",
    "            if (episode+1) % collect_rewards == 0:\n",
    "                average_reward = sum(rewards[-collect_rewards:])/collect_rewards\n",
    "                average_rewards.append(average_reward)\n",
    "    \n",
    "    return average_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d2523-d921-4dae-a400-a0ce4074ad66",
   "metadata": {},
   "source": [
    "#### Temporal Difference methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ef03c2c-98ac-41c0-8c35-407b9034dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_one_step(env, agent, episodes, show, save=None, collect_rewards=None, train=True):\n",
    "    \"\"\"\n",
    "    environment: instance of the environment to execute.\n",
    "    agent: instance of the agent that will interact with the environment.\n",
    "    show: integer. It indicates how often episodes are printed as text(it helps track the existence of bugs in the game itself).\n",
    "    save: integer. It indicates how often the V/Q-table is permanently persisted as a pickle object. None = no persistence at all.\n",
    "    collect_rewards: It indicates every so many episodes rewards are averaged. 1 = return every reward.\n",
    "    train: bool. If True, the agent learns by updating its V/Q-table; if False, the agent just executes a defined policy.\n",
    "    \n",
    "    returns: a list containing all average rewards computed throughout the whole experiment.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    average_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        if (episode+1) % show ==0:\n",
    "            print('Episode {0}:'.format(episode+1))\n",
    "            env.render()\n",
    "\n",
    "        state, reward, terminal, _ = env.reset()\n",
    "        while not terminal:\n",
    "            action = agent.follow_policy(state)\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            if train:\n",
    "                agent.evaluate_state(state, reward, terminal, action, next_state)   \n",
    "                \n",
    "            state=next_state\n",
    "            \n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if save:\n",
    "            if (episode+1) % save == 0:\n",
    "                agent.save_table()\n",
    "                \n",
    "        if collect_rewards:\n",
    "            if (episode+1) % collect_rewards == 0:\n",
    "                average_reward = sum(rewards[-collect_rewards:])/collect_rewards\n",
    "                average_rewards.append(average_reward)\n",
    "    \n",
    "    return average_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c931e90-163d-4ae2-996c-61f16ace79f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment_sarsa(env, agent, episodes, show, save=None, collect_rewards=None, train=True):\n",
    "    \"\"\"\n",
    "    environment: instance of the environment to execute.\n",
    "    agent: instance of the agent that will interact with the environment.\n",
    "    show: integer. It indicates how often episodes are printed as text(it helps track the existence of bugs in the game itself).\n",
    "    save: integer. It indicates how often the V/Q-table is permanently persisted as a pickle object. None = no persistence at all.\n",
    "    collect_rewards: It indicates every so many episodes rewards are averaged. 1 = return every reward.\n",
    "    train: bool. If True, the agent learns by updating its V/Q-table; if False, the agent just executes a defined policy.\n",
    "    \n",
    "    returns: a list containing all average rewards computed throughout the whole experiment.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    average_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        if (episode+1) % show ==0:\n",
    "            print('Episode {0}:'.format(episode+1))\n",
    "            env.render()\n",
    "\n",
    "        state, reward, terminal, _ = env.reset()\n",
    "        next_action = None\n",
    "        while not terminal:\n",
    "            if next_action:\n",
    "                action = next_action\n",
    "            else:\n",
    "                action = agent.follow_policy(state)\n",
    "                \n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            if not terminal:\n",
    "                next_action = agent.follow_policy(next_state)\n",
    "            else:\n",
    "                next_action = None\n",
    "                \n",
    "            if train:\n",
    "                agent.evaluate_state(state, reward, terminal, action, next_state, next_action)   \n",
    "            \n",
    "            state=next_state\n",
    "            \n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if save:\n",
    "            if (episode+1) % save == 0:\n",
    "                agent.save_table()\n",
    "                \n",
    "        if collect_rewards:\n",
    "            if (episode+1) % collect_rewards == 0:\n",
    "                average_reward = sum(rewards[-collect_rewards:])/collect_rewards\n",
    "                average_rewards.append(average_reward)\n",
    "    \n",
    "    return average_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec821cb-5506-4808-859c-e877554634cd",
   "metadata": {},
   "source": [
    "#### Wrapper:\n",
    "\n",
    "So far, I have deployed a total of 7 agents and 3 different experiments. I must admit that this can be somewhat confusing and needs to be untangled. Let's wrap  it all up in an informing dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d8e5b11-c845-4cdc-a394-853ce722cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_methods = {'MonteCarloPredictor': run_experiment,\n",
    "                  'MontecarloController': run_experiment,\n",
    "                  'OffPolicyMontecarlo': run_experiment,\n",
    "                  'TDLambdaPredictor': run_experiment_one_step,\n",
    "                  'WatkinsLambda': run_experiment_sarsa,\n",
    "                  'Sarsa': run_experiment_sarsa,\n",
    "                  'QLearning': run_experiment_one_step,\n",
    "                  'SarsaLambda': run_experiment_sarsa}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89278c2e-509a-4c8f-9c25-a13bea4e41a3",
   "metadata": {},
   "source": [
    "#### Train and test methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a06173dd-b674-457e-8841-8b2d06adb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_q_test(q, agent, episodes, show_every, save_every, collect_every):\n",
    "    \"\"\"\n",
    "    q: Queue from multiprocessing library\n",
    "    \"\"\"\n",
    "    q.put(agents_methods[agent.get_parent_class_str()](env, agent, episodes, show_every, save_every, collect_every, train=False))\n",
    "\n",
    "def get_q(q):\n",
    "    \"\"\"\n",
    "    q: Queue from multiprocessing library\n",
    "    \n",
    "    \"\"\"\n",
    "    return q.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d0bd42a-a799-4acc-a6f6-ed76c3bbaa35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_test(env, agent, test_until, test_every, sample_size=8_000, bootstrap_samples=1_000):\n",
    "    \"\"\"\n",
    "    env: rl environmen instance\n",
    "    agent: control agent instance\n",
    "    test_until: total number of episodes.\n",
    "    test_every: indicates how often evaluation is carried out. The agent's table is also saved then.\n",
    "    sample_size: how many episodes are averaged.\n",
    "    bootstrap_samples: how many samples of sample_size size are produced to determine the metric's CI.\n",
    "    \n",
    "    returns: a dictionary containing results, mean and percentiles for each evaluation\n",
    "    \"\"\"\n",
    "    def test(a):\n",
    "        \"\"\"\n",
    "        Parallelized execution\n",
    "        a: Prediction agent instance\n",
    "        \n",
    "        returns: a dictionary containing the whole list of results,\n",
    "                 the mean, the 2.5th percentile, and the 97.5 percentile.\n",
    "        \"\"\"\n",
    "        rewards = {'results':[]}\n",
    "        queue = mp.Manager().Queue()\n",
    "        cores = mp.cpu_count()\n",
    "    \n",
    "        #I split the whole test amongst the computer's cores:\n",
    "        EPISODES =  sample_size * bootstrap_samples // cores\n",
    "        SHOW_EVERY = 1_000_000\n",
    "        SAVE_EVERY =  None\n",
    "        COLLECT_EVERY = sample_size\n",
    "        \n",
    "        # I enqueue every run\n",
    "        for _ in range(0, cores):\n",
    "            mp.Process(target=put_q_test, args=(queue, a, EPISODES, SHOW_EVERY, SAVE_EVERY, COLLECT_EVERY,)).start()\n",
    "\n",
    "        getter = []  \n",
    "        #I create a pool of workers to handle dequeuing and subsequent execution:\n",
    "        pool = mp.Pool(cores)\n",
    "        for _ in range(0, cores):\n",
    "            getter.append(pool.apply_async(get_q, (queue,)))\n",
    "\n",
    "        #I extract results from every process and attach them to the dictionary:\n",
    "        for r in getter:\n",
    "            rewards['results'].extend(r.get())\n",
    "        \n",
    "        #statistics:\n",
    "        rewards['mean'] = np.array(rewards['results']).mean()\n",
    "        rewards['lower_bound'] = float(np.percentile(np.array(rewards['results']), [2.5]))\n",
    "        rewards['upper_bound'] = float(np.percentile(np.array(rewards['results']), [97.5]))\n",
    "        \n",
    "        return rewards\n",
    "            \n",
    "    summary = {}\n",
    "    for episodes in range(test_every, test_until + 1, test_every):\n",
    "        _ = agents_methods[agent.get_parent_class_str()](env, agent, test_every, 1_000_000, None, 1, True)\n",
    "        #table persistance:\n",
    "        agent.save_table()\n",
    "        #Instanciating a brand new prediction agent for each evaluation:\n",
    "        evaluator = Evaluator(env)\n",
    "        #Copy the Q-table to be tested:\n",
    "        evaluator.table = agent.table.copy()\n",
    "        summary[episodes] = test(evaluator)\n",
    "        \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e90c2-cbaf-46f5-96e7-f83de3e24e59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
