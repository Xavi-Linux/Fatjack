{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40616f6d-5eb0-49a3-a949-5d134061b679",
   "metadata": {},
   "source": [
    "# **Train-Test split**\n",
    "----\n",
    "\n",
    "## Goals of this notebook:\n",
    "\n",
    "1. Discuss the proper sample size to calculate average rewards (main metric).\n",
    "2. Define methods to alternate between train and test mode.\n",
    "3. Parallelize operations to gain some efficiency.\n",
    "---\n",
    "## Library imports\n",
    "\n",
    "#### 1. RL libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1d0cc2-c7ac-4a38-98b0-497db244bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import environments\n",
    "from agents.agents import TDLambdaPredictor, WatkinsLambda, Sarsa,\\\n",
    "                          QLearning, SarsaLambda, MonteCarloPredictor,\\\n",
    "                          MontecarloController, OffPolicyMontecarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f3ea1b-79cc-475e-9d6f-b2f9742e3375",
   "metadata": {},
   "source": [
    "#### 2. Data aggregation and matrix operation libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dcd437d-663d-4754-941b-2aceea018752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08411b0-82b1-46d9-bb62-2aa04858bee4",
   "metadata": {},
   "source": [
    "#### 3. Plotting libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e126c7d6-7dd8-4ef4-b764-e6c063e2616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib.ticker import FormatStrFormatter, ScalarFormatter, FuncFormatter\n",
    "import seaborn as sns\n",
    "# Commands to tweak notebook layout:\n",
    "from sys import maxsize\n",
    "np.set_printoptions(threshold=maxsize)\n",
    "plt.style.use('seaborn-pastel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747238e-58a1-4c74-95b0-deeb9b40c746",
   "metadata": {},
   "source": [
    "#### 4. Statistical analysis tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff03bd2-1d09-4a7b-81b5-1eb37c5e8561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.stats import normaltest, anderson, t, kstest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64926b-3ed8-405d-b8c2-515cbf5e4880",
   "metadata": {},
   "source": [
    "#### 5. Parallel programming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e38e041f-809e-4c0d-a283-0e4d69603e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cf2782-a589-4dde-9ac7-8715cdb74020",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Common plotting utilities:\n",
    "\n",
    "#### Average reward time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d849d99-e2e5-4d24-99be-73dde41b0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_time_series(X,Y, far=True):\n",
    "    \"\"\"\n",
    "    X: a range of numbers starting from the sample size number to the total number of episode + 1\n",
    "       (to make it inclusive) and incrementing by sample size steps.\n",
    "    Y: average rewards\n",
    "    far: bool. If False the plots zooms in.\n",
    "    \n",
    "    returns: the plot container instance\n",
    "    \"\"\"\n",
    "    # domain redefinition to adjust how many x-ticks are displayed dinamically\n",
    "    max_domain = X[-1]\n",
    "    multiplier = len(str(max_domain)) - 2\n",
    "    step = int(str(max_domain)[0]) * 10**multiplier\n",
    "    domain = np.arange(X[0], max_domain + step, step)\n",
    "    \n",
    "    if far:\n",
    "        ylim = (-1, 1)\n",
    "        yticks= np.arange(-1,1.05,0.05)\n",
    "    else:\n",
    "        max_value = max(0, max(Y))\n",
    "        ylim = (min(Y), max_value)\n",
    "        yticks = np.arange(min(Y), max_value + 0.01, 0.01)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    \n",
    "    style_dict = {'xlim': (X[0], max_domain),\n",
    "                  'xticks': domain,\n",
    "                  'xticklabels': domain,\n",
    "                  'xlabel': 'Episodes',\n",
    "                  'ylim': ylim,\n",
    "                  'yticks': yticks,\n",
    "                  'yticklabels': yticks,\n",
    "                  'title': 'Average reward over last {0:,.0f} episodes'.format(X[0])\n",
    "                 }\n",
    "    \n",
    "    ax = fig.add_subplot(1,1,1, **style_dict)\n",
    "    ax.grid()\n",
    "    #Axis formatters:\n",
    "    formatter = ScalarFormatter(useMathText=True)\n",
    "    #Scientific value display when the total number of episodes is over 1M:\n",
    "    formatter.set_scientific(True)\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('% 1.2f'))\n",
    "    # in and out of the money border:\n",
    "    ax.plot(domain.ravel(), np.zeros_like(domain).ravel(), color='red')\n",
    "    \n",
    "    ax.plot(X,Y)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718024d0-04ec-4a86-bdb1-b5dbbf3afadc",
   "metadata": {},
   "source": [
    "#### Density plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2397b259-2b2d-4e0a-b921-61a0212480cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density(data):\n",
    "    \"\"\"\n",
    "    data: a Panda Series containing every sample of average rewards\n",
    "    \n",
    "    returns: both histogram and boxplot container instances\n",
    "    \"\"\"\n",
    "    title = 'Average reward sampling: $\\overline{{X}}$={0:.2f}, s={1:.2f}, n={2:,.0f}, num of samples={3:,.0f}'\\\n",
    "            .format(data.mean(),\n",
    "                    data.std(),\n",
    "                    data.index.values[0],\n",
    "                    len(data))\n",
    "    \n",
    "    fig, (ax_box, ax_hist) = plt.subplots(2, sharex=True, figsize=(20,10), gridspec_kw={\"height_ratios\": (.15, .85)}) \n",
    "    fig.suptitle(t=title, fontsize=16, x=0.5, y=1.05)\n",
    "    \n",
    "    sns.boxplot(data=data,x=data, ax=ax_box)\n",
    "    sns.histplot(data=data, x=data, ax=ax_hist, bins=20, kde=True)\n",
    "    \n",
    "    #Delimiting tails:\n",
    "    ax_hist.axvline(x=np.percentile(data,[2.5]), label='2.5th percentile', c='r')\n",
    "    ax_hist.axvline(x=np.percentile(data,[97.5]), label='97.5th percentile', c='r')\n",
    "    ax_hist.legend()\n",
    "    ax_hist.set_xlabel('Average Reward')\n",
    "    \n",
    "    return ax_box, ax_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee627aed-eb74-4e8b-bf88-1f2538fe357c",
   "metadata": {},
   "source": [
    "#### Value Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6546cae-c81c-426d-980a-dc2aa753359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_v_func(table, title):\n",
    "    \"\"\"\n",
    "    table: agent's V-table (if agents has a Q-table, it must be transformed before being passed as an argument for this function)\n",
    "    title: str. A title for the plot\n",
    "    \n",
    "    returns: the plot container instance\n",
    "    \"\"\"\n",
    "    X = np.linspace(1,10,10)\n",
    "    Y = np.linspace(12, 20,9)\n",
    "    Xm, Ym = np.meshgrid(X, Y)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.suptitle(t=title, fontsize=16, x=0.5, y=1.05)\n",
    "    \n",
    "    common_style_dict = {'xlim': (X[0], X[-1]),\n",
    "                        'xticks': X,\n",
    "                        'xticklabels': ['{:.0f}'.format(value) for value in list(X)[1:]] + ['A'],\n",
    "                        'xlabel': 'Dealer\\'s Card',\n",
    "                        'ylim': (Y[0], Y[-1]),\n",
    "                        'yticks': Y,\n",
    "                        'yticklabels': Y,\n",
    "                         'ylabel': 'Player\\'s Total',\n",
    "                        'zlim': (-1, 1.5),\n",
    "                        'zticks': np.arange(-1, 1.8, 0.2),\n",
    "                        'zticklabels': np.arange(-1, 1.8, 0.2),\n",
    "                        }\n",
    "    \n",
    "    #Not usable ace-related states:\n",
    "    ax = fig.add_subplot(1,2,1, projection='3d', title='No usable Ace', **common_style_dict)\n",
    "    surface_1 = ax.plot_surface(Xm, Ym, table[8:17,:10,0], cmap=plt.get_cmap('bwr'), vmin=-1, vmax=1)\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('% 1.0f'))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('% 1.1f'))\n",
    "    ax.view_init(ax.elev, -120)\n",
    "    \n",
    "    #Usable ace-related states:\n",
    "    ax = fig.add_subplot(1,2,2, projection='3d', title=' Usable Ace', **common_style_dict)\n",
    "    surface_2 = ax.plot_surface(Xm, Ym, table[8:17,:10,1], cmap=plt.get_cmap('bwr'), vmin=-1, vmax=1)\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('% 1.0f'))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('% 1.1f'))\n",
    "    ax.view_init(ax.elev, -120)\n",
    "    \n",
    "    #Colour bar:\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.35, 0.01, 0.5])\n",
    "    fig.colorbar(surface_1,shrink=0.2, aspect=15, cax=cbar_ax)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e389bf90-f65b-4a9c-abf2-03832812f73f",
   "metadata": {},
   "source": [
    "#### Policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fabc8539-106c-4f7e-b3b2-a61e2d510594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_policy(table, title):\n",
    "    \"\"\"\n",
    "    table: matrix containing action indexes (it must have n-1 dimensions with respect to the agent's Q-table)\n",
    "    title: str. A title for the plot\n",
    "    \n",
    "    returns: the plot container instance\n",
    "    \"\"\"\n",
    "    #tick label transformer (used for the colour bar):\n",
    "    def format_tick(x, pos):\n",
    "        if x == 0:\n",
    "            return '{0}-STAND'.format(str(x))\n",
    "        elif x == 1:\n",
    "            return '{0}-HIT'.format(str(x))\n",
    "    \n",
    "    X = np.arange(-.5,9.5,1)\n",
    "    Y = np.arange(0.5, 11.5,1)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.suptitle(t=title, fontsize=16, x=0.5, y=1.05)\n",
    "    \n",
    "    common_style_dict = {'xlim': (-.5, 9.5),\n",
    "                        'xticks': X,\n",
    "                        'xticklabels': ['{:.0f}'.format(value) for value in range(2,11,1)] + ['A'],\n",
    "                        'xlabel': 'Dealer\\'s Card',\n",
    "                        'yticks': Y,\n",
    "                        'yticklabels': np.arange(11,22,1),\n",
    "                        'ylabel': 'Player\\'s Total'\n",
    "                        }\n",
    "    #I only plot those states wherein there is risk of going bust after hitting once more and, for the sake of symmetry,\n",
    "    #I manually modify those non-existing usable-ace-related states:\n",
    "    table[:8,:,1] = 1\n",
    "    \n",
    "    #Not usable ace-related states:\n",
    "    ax = fig.add_subplot(1,2,1, title='No usable Ace', **common_style_dict)\n",
    "    im = ax.imshow(table[7:18,:10,0], cmap=plt.get_cmap('bwr'), vmin=0, vmax=1, alpha=0.5)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True,color='w', linestyle='-', linewidth=1, axis='both')\n",
    "    ##Little tweak for tick labels' position:\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_horizontalalignment('left')\n",
    "\n",
    "    for label in ax.get_yticklabels():\n",
    "        label.set_verticalalignment('top')\n",
    "    \n",
    "    #Usable ace-related states:\n",
    "    ax = fig.add_subplot(1,2,2, title=' Usable Ace', **common_style_dict)\n",
    "    ax.imshow(table[7:18,:10,1],cmap=plt.get_cmap('bwr'), vmin=0, vmax=1, alpha=0.5)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True,color='w', linestyle='-', linewidth=1, axis='both')\n",
    "    ##Little tweak for tick labels' position:\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_horizontalalignment('left')\n",
    "    \n",
    "    #Colour bar definition\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    original_cmap = plt.get_cmap('bwr')\n",
    "    \n",
    "    #Converting a multiple-colour cmap into 2-colour cmap\n",
    "    cmap = ListedColormap([original_cmap(0), original_cmap(original_cmap.N)])\n",
    "    bounds = [0, 0.5, 1]\n",
    "    norm = BoundaryNorm(bounds, original_cmap.N)    \n",
    "    cbar_ax = fig.add_axes([0.85, 0.05, 0.01, 0.85])\n",
    "    \n",
    "    fig.colorbar(cm.ScalarMappable(cmap=cmap, norm=norm),\n",
    "                 shrink=0.2,\n",
    "                 aspect=15,\n",
    "                 cax=cbar_ax,\n",
    "                 boundaries= [0] + bounds + [2],  \n",
    "                 extend='both',\n",
    "                 ticks=[0,1],\n",
    "                 format = FuncFormatter(format_tick),\n",
    "                 spacing='proportional',\n",
    "                 orientation='vertical',\n",
    "                 alpha= 0.5)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87895b41-ff22-4324-b6ba-3fbc13473f2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Statistical functions:\n",
    "\n",
    "I use an **alpha value of 1%** for any statistical test executed in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92ddf5fd-27af-4c6b-83ee-5e844115fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE_STRING_FORMAT = '1%'\n",
    "CONFIDENCE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ff10b6f-56df-44a4-bf45-52ce60cfbbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dickey_fuller(time_series, confidence=CONFIDENCE):\n",
    "    fuller_results = adfuller(time_series)\n",
    "    if fuller_results[1] <= confidence:\n",
    "        return 'Null hypothesis can be rejected. The series is stationary.'\n",
    "    else:\n",
    "        return 'Null hypothesis cannot be rejected. The series is not stationary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "a78f7c18-92b1-42c4-b1e2-7bc715336a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kolmogorov_smirnov(values, val):\n",
    "    standarised = StandardScaler().fit(values.reshape((len(values),1)))\\\n",
    "                                  .transform(values.reshape((len(values),1)))\\\n",
    "                                  .ravel()\n",
    "        \n",
    "    return int(kstest(standarised, t.rvs(val-1,size=len(values))).pvalue > alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b316980-ade0-443e-96a8-f816c4a0f436",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Experiment Definition:\n",
    "\n",
    "#### Montecarlo method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bd4e7ad-8898-4607-88e2-e9a4f144a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(environment, agent, episodes, show, save=None, collect_rewards=None, train=True):\n",
    "    \"\"\"\n",
    "    environment: instance of the environment to execute.\n",
    "    agent: instance of the agent that will interact with the environment.\n",
    "    show: integer. It indicates how often episodes are printed as text(it helps track the existence of bugs in the game itself).\n",
    "    save: integer. It indicates how often the V/Q-table is permanently persisted as a pickle object. None = no persistence at all.\n",
    "    collect_rewards: It indicates every so many episodes rewards are averaged. 1 = return every reward.\n",
    "    train: bool. If True, the agent learns by updating its V/Q-table; if False, the agent just executes a defined policy.\n",
    "    \n",
    "    returns: a list containing all average rewards computed throughout the whole experiment.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    average_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        if (episode+1) % show ==0:\n",
    "            print('Episode {0}:'.format(episode+1))\n",
    "            env.render()\n",
    "\n",
    "        state, reward, terminal, _ = env.reset()\n",
    "        while not terminal:\n",
    "            action = agent.follow_policy(state)\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            if train:\n",
    "                agent.evaluate_state(state, reward, terminal, action)   \n",
    "                \n",
    "            state=next_state\n",
    "            \n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if save:\n",
    "            if (episode+1) % save == 0:\n",
    "                agent.save_table()\n",
    "                \n",
    "        if collect_rewards:\n",
    "            if (episode+1) % collect_rewards == 0:\n",
    "                average_reward = sum(rewards[-collect_rewards:])/collect_rewards\n",
    "                average_rewards.append(average_reward)\n",
    "    \n",
    "    return average_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd5873f4-ed5e-45ac-b3f0-7f0f34e45d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_one_step(env, agent, episodes, show, save=None, collect_rewards=None, train=True):\n",
    "    \"\"\"\n",
    "    environment: instance of the environment to execute.\n",
    "    agent: instance of the agent that will interact with the environment.\n",
    "    show: integer. It indicates how often episodes are printed as text(it helps track the existence of bugs in the game itself).\n",
    "    save: integer. It indicates how often the V/Q-table is permanently persisted as a pickle object. None = no persistence at all.\n",
    "    collect_rewards: It indicates every so many episodes rewards are averaged. 1 = return every reward.\n",
    "    train: bool. If True, the agent learns by updating its V/Q-table; if False, the agent just executes a defined policy.\n",
    "    \n",
    "    returns: a list containing all average rewards computed throughout the whole experiment.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    average_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        if (episode+1) % show ==0:\n",
    "            print('Episode {0}:'.format(episode+1))\n",
    "            env.render()\n",
    "\n",
    "        state, reward, terminal, _ = env.reset()\n",
    "        while not terminal:\n",
    "            action = agent.follow_policy(state)\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            if train:\n",
    "                agent.evaluate_state(state, reward, terminal, action, next_state)   \n",
    "                \n",
    "            state=next_state\n",
    "            \n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if save:\n",
    "            if (episode+1) % save == 0:\n",
    "                agent.save_table()\n",
    "                \n",
    "        if collect_rewards:\n",
    "            if (episode+1) % collect_rewards == 0:\n",
    "                average_reward = sum(rewards[-collect_rewards:])/collect_rewards\n",
    "                average_rewards.append(average_reward)\n",
    "    \n",
    "    return average_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e54bac99-c754-4e34-b2d9-ee453aa8cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_sarsa(env, agent, episodes, show, save=None, collect_rewards=None, train=True):\n",
    "    \"\"\"\n",
    "    environment: instance of the environment to execute.\n",
    "    agent: instance of the agent that will interact with the environment.\n",
    "    show: integer. It indicates how often episodes are printed as text(it helps track the existence of bugs in the game itself).\n",
    "    save: integer. It indicates how often the V/Q-table is permanently persisted as a pickle object. None = no persistence at all.\n",
    "    collect_rewards: It indicates every so many episodes rewards are averaged. 1 = return every reward.\n",
    "    train: bool. If True, the agent learns by updating its V/Q-table; if False, the agent just executes a defined policy.\n",
    "    \n",
    "    returns: a list containing all average rewards computed throughout the whole experiment.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    average_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        if (episode+1) % show ==0:\n",
    "            print('Episode {0}:'.format(episode+1))\n",
    "            env.render()\n",
    "\n",
    "        state, reward, terminal, _ = env.reset()\n",
    "        next_action = None\n",
    "        while not terminal:\n",
    "            if next_action:\n",
    "                action = next_action\n",
    "            else:\n",
    "                action = agent.follow_policy(state)\n",
    "                \n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            if not terminal:\n",
    "                next_action = agent.follow_policy(next_state)\n",
    "            else:\n",
    "                next_action = None\n",
    "                \n",
    "            if train:\n",
    "                agent.evaluate_state(state, reward, terminal, action, next_state, next_action)   \n",
    "            \n",
    "            state=next_state\n",
    "            \n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if save:\n",
    "            if (episode+1) % save == 0:\n",
    "                agent.save_table()\n",
    "                \n",
    "        if collect_rewards:\n",
    "            if (episode+1) % collect_rewards == 0:\n",
    "                average_reward = sum(rewards[-collect_rewards:])/collect_rewards\n",
    "                average_rewards.append(average_reward)\n",
    "    \n",
    "    return average_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d03fb-4696-4ea9-8dd2-5424f7d6003b",
   "metadata": {},
   "source": [
    "#### Wrapper:\n",
    "\n",
    "So far, I have deployed a total of 7 agents and 3 different experiments. I must admit that this can be somewhat confusing and needs to be untangled. Let's wrap  it all up in an informing dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a78c2142-ca49-4a97-afec-f60a9a5b76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_methods = {'MonteCarloPredictor': run_experiment,\n",
    "                  'MontecarloController': run_experiment,\n",
    "                  'OffPolicyMontecarlo': run_experiment,\n",
    "                  'TDLambdaPredictor': run_experiment_one_step,\n",
    "                  'WatkinsLambda': run_experiment_sarsa,\n",
    "                  'Sarsa': run_experiment_sarsa,\n",
    "                  'QLearning': run_experiment_one_step,\n",
    "                  'SarsaLambda': run_experiment_sarsa}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b987d4d8-ec13-4e83-a4c6-a8fc9646c9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.run_experiment(environment, agent, episodes, show, save=None, collect_rewards=None, train=True)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents_methods['MonteCarloPredictor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30df195-dba8-4f32-be82-2c84bf07b2d6",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## On Sample Size (to evaluate policies):\n",
    "\n",
    "So far, I have just used an arbitrary number to decide how often rewards should be averaged. However, it would be nice to find a good balance between finding a good estimation of the real value of a policy and execution time.\n",
    "\n",
    "It seems obvious that the larger the sample size, the lower the standard deviation. So, I will empirically assess how standard deviation behaves when enlarging the sample size and then decide the sample size I will use for future experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faa4ef0e-748a-4d0d-84fa-bf7caf8497c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an environment instance:\n",
    "env = environments.make('hitstand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2260e9e2-f97d-417b-851c-b4e634de34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an agent that uses an arbitrary policy to play (policy is not important for the problem at hand)\n",
    "class Deterministic(MonteCarloPredictor):\n",
    "    #V-table is initialized full of 0s\n",
    "    def follow_policy(self, observation, *args):\n",
    "        #if your cards add up to a number greater than 17, stand; otherwise, hit:\n",
    "        if observation[0] > 17:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6fffe-abb3-4e01-aa51-8ca3c96a869e",
   "metadata": {},
   "source": [
    "I set the ground for a parallel execution by defining two functions: the first one enqueues RL experiments and the second ones dequeues them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "121a08f0-bcad-4aa7-8ee1-0bfe6c91e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_q(q, episodes, show_every, save_every, collect_every):\n",
    "    \"\"\"\n",
    "    q: Queue from multiprocessing library\n",
    "    \"\"\"\n",
    "    sampler = Deterministic(env)\n",
    "    q.put(agents_methods[sampler.get_parent_class_str()](env, sampler, episodes, show_every, save_every, collect_every, train=False))\n",
    "    \n",
    "def get_q(q):\n",
    "    \"\"\"\n",
    "    q: Queue from multiprocessing library\n",
    "    \n",
    "    \"\"\"\n",
    "    return q.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "053a0fd6-4683-4776-8e31-3b48fd279506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_sample_size_choice(sample_sizes, num_of_samples=10, number_of_runs=10):\n",
    "    \"\"\"\n",
    "    sample_sizes: a list of integers.\n",
    "    num_of_samples: an integer to indicate how many times rewards are averaged at\n",
    "                    each run. It helps simulate a bootstrapping process.\n",
    "    number_of_runs: number of times an experiment is repeated for each sample size.\n",
    "    \n",
    "    returns: a dictionary containing the reported stds and means for each run and sample size\n",
    "             dict[sample_size] = {stds: [std_run_1, std_run_2 ....],\n",
    "                                  means: [mean_run_1, mean_run_2 ....]\n",
    "                                  }\n",
    "    \"\"\"\n",
    "    summary = {}    \n",
    "    for sample_size in sample_sizes:\n",
    "        summary[sample_size] = {'stds': [],\n",
    "                                'means': [],\n",
    "                                }\n",
    "        # I use a server (Manager()) to allow the Queue to interact with different processes\n",
    "        queue = mp.Manager().Queue()\n",
    "        \n",
    "        EPISODES =  sample_size * num_of_samples\n",
    "        SHOW_EVERY = 1_000_000\n",
    "        SAVE_EVERY =  None\n",
    "        COLLECT_EVERY = sample_size\n",
    "        \n",
    "        # I enqueue every run\n",
    "        for _ in range(0, number_of_runs):\n",
    "            mp.Process(target=put_q, args=(queue, EPISODES, SHOW_EVERY, SAVE_EVERY, COLLECT_EVERY,)).start()\n",
    "\n",
    "        getter = []  \n",
    "        #I create a pool of workers to handle dequeuing and subsequent execution:\n",
    "        pool = mp.Pool(8)\n",
    "        for _ in range(0, number_of_runs):\n",
    "            getter.append(pool.apply_async(get_q, (queue,)))\n",
    "\n",
    "        #I extract results from every process and attach them to the dictionary:\n",
    "        for r in getter:\n",
    "            np_results = np.array(r.get())\n",
    "            summary[sample_size]['stds'].append(np_results.std())\n",
    "            summary[sample_size]['means'].append(np_results.mean())\n",
    "        \n",
    "    \n",
    "    return summary            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d846b-6003-4aad-8bae-b03ddac9b265",
   "metadata": {},
   "source": [
    "`This execution might take several minutes`\n",
    "1. I define some sample sizes.\n",
    "2. For each of them, I run 10 experiments.\n",
    "3. Ten average rewards are generated at each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "3513434a-e562-40d8-88ea-8c85d9f18270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_size_options=[100, 250, 500, 750, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "sample_size_summary = wrap_sample_size_choice(sample_size_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "73b08c08-94eb-4c3d-bf4e-f1d00087da38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stds = []\n",
    "for sample_size in sample_size_summary:\n",
    "    stds.append(sample_size_summary[sample_size]['stds'])\n",
    "plt.boxplot(stds, labels=sample_size_summary.keys())\n",
    "plt.gca().set_ylim(0,);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
